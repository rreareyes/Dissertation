---
title : ""

---

```{r setup, include = FALSE}
library(papaja)
library(kableExtra)
library(tidyverse)
library(gt)
library(english)

```

```{r}
knitr::opts_chunk$set(warning=FALSE, message=FALSE, echo=FALSE) 
```

```{r base-paths}
folder_root <- dirname(dirname(rstudioapi::getActiveDocumentContext()$path))

folder_helpers   <- file.path(folder_root, "bin", "helpers")
folder_fits      <- file.path(folder_root, "results", "fits")
folder_summaries <- file.path(folder_root, "results", "summaries")
folder_keys      <- file.path(folder_root, "results", "keys")
folder_figures   <- file.path(folder_root, "figures")

# Load reporting functions
source(file = file.path(folder_helpers, "h01_report.R"))

```

```{r stats-and-summaries}
load(file.path(folder_summaries, "ch02_summary_demographics.RData"))
load(file.path(folder_summaries, "ch02_bayes_rope.RData"))
load(file.path(folder_summaries, "ch02_reported_anovas.RData"))
load(file.path(folder_keys, "ch02_accepted_performance_key.RData"))
load(file.path(folder_keys, "ch02_discarded_performance_key.RData"))
load(file.path(folder_keys, "ch02_collected_participants_key.RData"))
```

# CHAPTER II {.unlisted .unnumbered}
# STUDY 1: EYE-TRACKING ANALYSIS OF HEURISTIC-DRIVEN INFORMATION SEARCH IN UNCERTAIN DECISION ENVIRONMENTS {.unlisted .unnumbered}

\begin{adjustwidth}{2cm}{}
\hspace{\parindent}\emph{"Maturity, one discovers, has everything to do with the acceptance of not knowing."}

\begin{flushright}
--- Mark Z. Danielewski,  \emph{House of Leaves}
\end{flushright}
\end{adjustwidth}

Choosing between two options (e.g., ice cream or cake for dessert, wearing a white or blue shirt to a job interview, getting a cat or a dog as a pet, etc.), is arguably one of the most common decision scenarios we face in our daily lives. 
At first, decisions for such binary choices seem fairly easy. 
However, we can also imagine scenarios where these decisions become difficult as you keep adding on more attributes to consider: the location, number of rooms, flooring type, and cost when buying a house; weighing the coverage, premiums and deductibles when selecting a health plan; reviewing genre, ratings and synopsis while scrolling through your favorite streaming app. 
These situations show how real-life decision environments require processing multiple attributes comprehensively prior to making a choice.

One of the most widely studied decision scenarios in the literature on decision strategies and heuristics use is the binary choice paradigm \citep{broder_2000_assessing_empirical, broder_2003_decision_making, dieckmann_rieskamp_2007_influence_information, lee_cummins_2004_evidence_accumulation, rieskamp_hoffrage_1999_when_people}. 
In these tasks, decision makers are presented with two options which have a certain number of attributes to be evaluated upon. 
Each attribute tends to be binary, i.e., having two possible states, but only one state can be present at a time. 
These binary features vary in terms of validity, i.e., the relative importance in predicting the best option.
Depending on the combination of attributes and their states, one option becomes more desirable than the other.

The decision strategies identified from binary choice paradigms can be broadly categorized into compensatory strategies, where all information is processed exhaustively with trade-offs being made between them, and non-compensatory strategies, where not all available information is used and trade-offs between them are often ignored  \citep{martignon_hoffrage_1999_why_does, rieskamp_hoffrage_1999_when_people}. 

A good example of a compensatory strategy is the linear integration of all pieces of information, which is also referred to as the Weighted Additive (WADD) model, \citep{payne_etal_1988_adaptive_strategy}, or the Franklin rule \citep{gigerenzer_goldstein_1999_betting_one}. 
This strategy integrates all pieces of information by first determining the winning state within each attribute. 
Then it multiplies each attribute's state by their validity to finally add them up within an option. 
The option with higher weighted sum across attributes is selected. 
Despite this strategy being a highly successful way to approach even the most complex decision scenarios, its correct implementation involves an extensively complicated procedure, which taxes the agent's cognitive resources.

Another well studied compensatory strategy is the Dawes rule  \citep{dawes_corrigan_1974_linear_models}. 
This method also considers all pieces of information, but instead of assigning different subjective validities, it equally weighs all the attributes. 
In the end, the agent sums the number of positive states within each alternative (i.e., tallying the "good" attributes) and chooses the option with a higher tally score. This strategy is also fairly successful across all decision environments since it removes the requirement to rank the different features.

Among non-compensatory strategies, the heuristic known as Take-The-Best (TTB) \citep{gigerenzer_goldstein_1996_reasoning_fast} stands out for its information sampling efficiency. 
Similar to WADD, TTB assumes that decision attributes vary in terms of validity. 
Critically, TTB ranks the attributes based on their validity and begins searching for information from most to least important, until a discriminating feature (i.e., presence of a winning state) allows the agent to distinguish between the options. In the end, TTB relies on this single discerning feature to make a choice. 
This strategy is very robust and surpasses or matches the accuracy of analytic compensatory strategies, such as WADD in many decision scenarios \citep{czerlinski_etal_1999_how_good}. 

While the above described decision strategies and heuristics provide reasonable explanations on how the information can be obtained, processed, and implemented to make choices, the underlying assumptions for each strategy have not been verified. 
This is partly due to the fact the pre-decisional processes are non-observable cognitive constructs that can only be inferred by modeling of the observed choices or indirect measures that reflect processing of information \citep{harte_koele_2001_modelling_describing}.
In recent years, several different approaches of process-tracing have been used to study the pattern of pre-decisional information search. Process tracing can be implemented, for example, by asking the participant to flip cards \citep{verplanken_weenig_1993_graphical_energy}, click with a mouse \citep{payne_etal_1988_adaptive_strategy, rieskamp_otto_2006_ssl_theory,  van_ravenzwaaij_etal_2014_hierarchical_bayesian} or pay fees to acquire information (e.g., \citealt{broder_2000_assessing_empirical, newell_shanks_2003_take_best, rieskamp_otto_2006_ssl_theory}). 
These methods have the disadvantage of constraining the frequency of comparisons and limiting information search \citep{glockner_betsch_2008_multiplereason_decision} due to the additional costs for acquiring information. 
More recently, eye-tracking has been widely adopted in decision making research, which provides account on pre-decisional information processing based on the analysis of fixation patterns (e.g., \citealt{ashby_etal_2016_applications_innovations,  callaway_etal_2021_fixation_patterns, orquin_mueller_loose_2013_attention_choice}). 
This method allows to display simultaneously all the relevant visual information from the decision alternatives and record the fixation patterns while the participant freely searches for information, thus providing a more naturalistic measure of information processing.

In this study we investigated the decision strategies and heuristics used by participants under uncertain scenarios. 
We also examined the patterns of pre-decisional information processing within these strategies with the aim of verifying their underlying assumptions. 
To this end, we presented naive participants with a two alternative multi-attribute probabilistic classification task while we recorded their eye movements with an eye-tracker. 
This design emulates the uncertainty in real life decision making scenarios and avoids memorization of cue combinations. 
Our task design emulates the uncertainty in real-life decision-making scenarios, where a choice is made based on multiple attributes that collectively predict the preferable option. 
In environments like this, participants rely on their own subjective estimates for the validities of each attribute \citep{oh-descher_etal_2017_probabilistic_inference, oh_etal_2016_satisficing_splitsecond}. 
We created decision models that predict choice at the trial level, based on the combination of states from the multiple attributes. 
We then fitted these models to each participant's choices to classify them according to the decision strategies they use via model comparison. 
Then, we studied the fixation patterns of the participants classified under each strategy to test if they followed the expected pattern of fixations across the cue attributes for that particular strategy. 
Finally, we compared the choice outcomes (i.e., classification accuracy) and fixation patterns across decision strategies to characterize their differences.

## Methods
### Participants
We collected a total of `r printnum(length(collected_participants_key$subject))` participants from the student population of the University of Massachusetts, Amherst.
We excluded `r printnum(length(discarded_performance_key$subject))` participants who performed at chance level during the task and one participant due to technical problems with the eye-tracking device.
This left us with a total of `r printnum(sum(age_summary$n))` participants. 
From these  `r age_summary %>% filter(experiment == "1") %>% .[[1, 4]] %>% words()` individuals participated in Experiment 1 (E1, `r printnum(female_summary %>% .[[1, 5]])` females; mean age = `r printnum(age_summary %>% .[[1, 2]])`, SD = `r printnum(age_summary %>% .[[1, 3]])`) and `r age_summary %>% filter(experiment == "1") %>% .[[1, 4]] %>% english()` participated in Experiment 2 (E2, `r printnum(female_summary %>% .[[2, 5]])` females; mean age = `r printnum(age_summary %>% .[[2, 2]])`, SD = `r printnum(age_summary %>% .[[2, 3]])`). 
We compensated all of them with a flat rate of class credit or \$12, as well as a monetary bonus of \$5 based on task performance. 
All participants provided informed consent in line with the Institutional Review Board from the University of Massachusetts, Amherst.

### Two-Alternative Multi-Attribute Probabilistic Classification Task
An overview of the task is shown in Fig. \ref{fig:ch2-task-setup}. 
We adapted the probabilistic classification paradigm used in \citet{oh_etal_2016_satisficing_splitsecond} for eye-tracking purposes. 
This paradigm carries many common features of the widely used binary choice paradigms (e.g., \citealt{broder_2000_assessing_empirical, broder_2003_decision_making, dieckmann_rieskamp_2007_influence_information, lee_cummins_2004_evidence_accumulation, rieskamp_otto_2006_ssl_theory}). 
On each trial, participants chose between two alternatives, based on the information provided by four features that varied in terms of how well they predicted the winning option. 
We used the same general design for both E1 and E2, except for the second testing phase (T2) which we will describe in detail below. 

We framed this task as a race between two pilots who carried four different pieces of equipment, each with two possible brands. 
We presented the combination of these equipment brands from each pilot as a 2 by 2 grid on the left and right side of the screen. 
Each cell on the grid represented a different equipment item, which had binary states in the form of two possible brands: "JU" or "JO" for the helmet; "NE" or "NI" for the fin; "BE" or "BA" for the skates; and "WO" or "WI" for the wheels. 
We displayed the brands written on the center of each cell, with a faded visual image of the corresponding equipment in the background. 
To make the task more suitable for eye tracking, we took measures to ensure that the participants fixated directly on the brand to extract information from it. 
First, the two brand names for each equipment only varied in terms of a vowel, which made it hard for visual identification unless they made a proper fixation. 
Secondly, we surrounded the brand names with hashes to limit the possibility of using peripheral vision instead of fixating on them.     

We created a set of stimuli using all combinations of cue states (i.e., the brands), giving us a total of sixteen unique grids ($4^2 = 16$). 
Additionally, we assigned complementary probabilistic weights to the two states of each item (see Fig. \ref{fig:ch2-task-setup} for weights assigned). 
These weights predicted each pilot's probability of winning. The absolute difference between the two complementary weights assigned to each equipment is an objective measure of its validity, i.e., how good a piece of equipment was at predicting the winning pilot. 
The larger this difference, the higher their relative importance compared to the other items. 
The location of the four items on the grid was consistent across participants, however we randomly assigned the complementary pairs of weights to different equipment for each individual, covering the 24 ($4!=24$) possible permutations of weights across all participants.

On each trial, we presented the grids in pairs. 
We created these pairs by combining the complete set of sixteen grids with each other. 
This yielded a total of 120 unique pairs of grid cues ($\frac{16^{2}}{2} - \frac{16}{2}=120$) and 240 trials in total by mirroring the left and right position of the cues on the screen. 
We asked participants to compare the displayed cues, using the information from the equipment brands to predict which pilot was more likely to win the trial. 

We calculated the probability that the left ($P(L|C_L, C_R)$) or the right ($P(R|C_L, C_R)$) pilot wins, by comparing the weights of the equipment states in the left cue ($W_L = \{w_{C_{L,1}}, ... w_{C_{L,4}}\}$) against the ones in the right cue ($W_R = \{w_{C_{R,1}}, ... w_{C_{R,4}}\}$) using the following equations: 

$$P(L|C_L, C_R) = \frac {10^{\sum_{i=1}^ {4} (w_{c_{L,i}} - w_{c_{R,i}})}} {1+10^{\sum_{i=1}^ {4} (w_{c_{L,i}} - w_{c_{R,i}})}}$$

$$P(R|C_L, C_R) = 1 - P(L|C_L,C_R)$$

Where *i* represents the *i*th item inside each cue *c* ($C_L:$ left cue, $C_R:$ right cue). 

Participants had up to 15 seconds to choose by pressing the left and right arrow key on the keyboard, using their right hand (Fig. \ref{fig:ch2-task-setup}B). 
Upon making their choice or after the 15 seconds expired, the cues disappeared from the screen. 
After this, we displayed a feedback message for 500 ms, informing them about the outcome of their choice (i.e., "win" or "lose" indicating whether the chosen pilot won or lost) or a "miss" message in case of no response. The "win" or "lose" outcomes were determined based on the probability of winning for each side. 
That is, to determine the outcome, we compared the probability of winning for the chosen side (i.e., *P(L)* or *P(R)*) to a random number drawn from a uniform distribution between 0 and 1. 
If the probability of the cue selected by the participant was larger or smaller than this random number, they received a "win" or "lose" outcome, respectively. 
Under these circumstances, participants could receive a "lose" message, despite choosing the cue with higher winning probability.

As part of feedback, participants also received reward points based on response time. 
We used these rewards to incentivize heuristics use by encouraging participants to make faster responses. 
We imposed an adaptive time pressure based on the moving average ($\mu$RT) reaction time up until the previous trial within each participant. 
Winning trials with RT within the range of Â±1SD from the ($\mu$RT) were granted 1 point, while faster (RT < $\mu$RT - 1$\sigma$RT) or slower (RT > $\mu$RT + 1$\sigma$RT) responses granted 2 points or 0 points respectively (Fig. \ref{fig:ch2-task-setup}C). 
We assigned an ITI of 1 second between the feedback screen and the presentation of the next trial during which participants fixated on the center of the screen.

Before the start of the experiment, we presented the task instructions as a slide show, after which we asked the participants to provide us with a brief explanation of the task. 
Regardless of their answer we always explained the following:

>*In this task, you have to choose between two pilots presented on the left and right side of the screen. Each pilot will be represented by a grid with the equipment they are using, and you will choose the one you think has a higher chance of winning by pressing the corresponding arrow key with your right hand. You will have up to 15 seconds to make your choice. At the beginning you don't know anything about what brands of equipment are the best, but you will learn as the task advances by trial and error. Just try to pay attention to what combination of brands are making you win more often. As you read before, if you win the race and you answer fast you will get an extra point. This fast threshold is determined by your performance, that means the computer will adapt to your speed. There are a total of eight blocks, and you will have breaks in between each of them. Finally, I am going to ask you to please keep your right hand on the keyboard during the whole time you are doing the task.*

In both E1 and E2, participants completed a total of 480 trials, organized into 8 blocks of 60 trials across 3 phases: a practice phase (P1) and two testing phases (T1 and T2). 
During P1 we presented participants with two full sets of unique cue pairs (240 trials in total) across 4 blocks, allowing them to develop a decision strategy. 
During both T1 and T2, we presented 120 unique pairs divided into two blocks. 
For T1, the contingencies of probabilistic weights to the cue states remained the same as P1. 
During T2 however, we introduced changes to these contingencies, unbeknownst to the participants (see Fig. \ref{fig:ch2-task-setup}A for the shift in probabilistic weights in T2). 
For E1, we set the complementary weights for the two states to 0.80/0.20 across all items. 
For E2 we inverted the ranking of the features, making the most informative item (i.e., weights 0.95/0.05) become the least informative (i.e., weights 0.5001/0.4999) and so on.
In both E1 and E2, we kept the winning states from P1 and T1 the same in T2.
We introduced these new contingencies in T2 as a way of dissociating the decision strategies developed and used during P1 and T1 by looking at the performance (i.e., choice accuracy) change from T1 to T2. 
For example, in E1, the performance in T2 would be better for participants who adopted a compensatory strategy of equally considering all attributes, compared to those who have been using frugal strategies relying on fewer number of attributes, since all four equipment items had the same probabilistic weights. 
In E2, the new contingencies in T2 would particularly hurt users of strategies relying on ranking attributes (e.g., TTB), whereas those using a strategy based on counting positive attributes without an explicit ranking (e.g., Tallying) should be resilient to the change. 
At the end of the task, the participants could obtain a monetary bonus of $5 based on the total amount of points they earned throughout the session. 
We set an arbitrary threshold at 400 points, which was calculated based on winning 70% of the trials with 30% of them being fast responses. 

We built and presented the task using the Psychophysics Toolbox extension \citep{brainard_1997_psychophysics_toolbox} in MATLAB. 
Throughout the task, we used a SMI RED250mobile eye tracker (SensoMotoric Instruments) and the SMITE toolbox \citep{niehorster_nystrom_2020_smite_toolbox} to record participant's fixation patterns.

### Data Analysis
The focus of our analysis was to identify the naturally developing decision strategies under a complex decision-making scenario and determine whether their pre-decisional information search patterns aligned with the assumptions for each decision strategy. 
To achieve this goal, we first classified our participants as users of different decision strategies based on their choices using model classifications. 
We then compared the proportion of correct responses during T1 and T2 to confirm the dissociation between decision strategies. 
Finally, we investigated the fixation patterns in each decision strategy based on the eye-tracking data.

#### Model Classification
We implemented model classification using variational Bayesian inference \citep{drugowitsch_2019_variational_bayesian, oh-descher_etal_2017_probabilistic_inference, oh_etal_2016_satisficing_splitsecond}. 
This approach allows us to compare the support that different decision models have for each participant, given the choices made in each trial. 
For this we used only the trials from T1, since we considered that by this time point the participants should have established a consistent decision strategy. 

Given the binary nature of the decisions made during the task, we created logistic models to fit the responses from the participants. 
These models estimate the likelihood of a participant choosing the left cue, given the support from the different brands presented in the grid:

$$choice_{i} \sim \beta_{0} + \beta_{n} \text{F}_{n}$$


where $choice_{i}$ represents the side selected by the participant in the *i*th trial, which we coded as 1 (left) or 0 (right). 
$F_n$ represents the state (i.e., equipment brand) from the *n*th feature (i.e., equipment) considered by the decision model. 
$F_n$ can take the value of 1 if the winning state was located in the left cue, 0 if both cues had the same state, and -1 if the winning state appeared in the right cue. 
The decision models had different number of predictors, depending on the number of features being considered. 
Similar to \citet{oh_etal_2016_satisficing_splitsecond}, we first defined an optimal model with four predictors:

$$choice_{i} \sim \beta_{0} + \beta_{1} \text{F}_{1} + \beta_{2} \text{F}_{2} + \beta_{3} \text{F}_{3} + \beta_{4} \text{F}_{4}$$

This model represents the integration of the four features from the cue allowing each to have their own weight in the decision process (model 15 in Fig. \ref{fig:ch2-model-configuration}, which is equivalent to WADD). 
We used this as the reference to contrast all the other decision models, considering that it is the most flexible model to fit. 
We also created 14 models that represent different integration of features: four single parameter models, which represent the use of a single feature to make a  choice (Fig. \ref{fig:ch2-model-configuration}, models 1-4); six models that integrate two features (Fig. \ref{fig:ch2-model-configuration}, models 5-10) and four models that integrate three features (Fig. \ref{fig:ch2-model-configuration}, models 11-14).

Additionally, we considered the possibility that the participants would use a compensatory strategy like Tallying to solve this task. 
This strategy merely counts the number of positive attributes present in each cue (i.e., winning states/brands in the context of this task), adds them up, and chooses the option with the larger number. 
To represent this strategy, we created a model with a single predictor that could take any integer from -4 to 4, where negative values indicate number of features favoring the right side, while positive values represent the number of features in favor of the left cue (Fig. \ref{fig:ch2-model-configuration}, model 16). 
This way, the model uses information from all the features, but they are merged in a single predictor.

Next, we wanted to represent the choice patterns expected from users of the TTB strategy. 
To do this, we created a model with 4 different predictors that compares the two states from the same feature in each cue, starting from the most important one, and going down in descending order of importance, until a choice is made when it identifies a difference in the two states (Fig. \ref{fig:ch2-model-configuration}, model 19). 
A trial where the most important feature was favoring the left side would have $[1, 0, 0, 0]$ as the vector of predictors, whereas a trial where the discerning feature favoring the right cue was located in the third most important item would take the form $[0, 0, -1, 0]$. 
This decision model would technically be relying on a single feature in each trial to make a choice, but it would vary in how many features it would have to review before choosing, depending on the particular set of cues being compared.

Finally, to account for models implemented in an incomplete fashion, we created the models representing partial Tallying (Fig. \ref{fig:ch2-model-configuration}, model 17-18) and incomplete TTB (Fig. \ref{fig:ch2-model-configuration}, model 20-21). 
The specific formulas from each model listed in Fig. \ref{fig:ch2-model-configuration} are included in Appendix A.

To classify participant's choice patterns into one of the decision models, we calculated the Bayes factors ($BF_m$) of each individual by comparing the probability that the choice data was generated by the decision model ($P(Data|Model_{m})$) in comparison to the reference model ($P(Data|Model_{ref})$):

$$BF_{m} = \frac{P(Data|Model_{m})}{P(Data|Model_{ref})}$$

We considered models with a $BF_m > 3$ as having more evidence in their favor compared to the reference model[@kass_raftery_1995_bayes_factors]. 
If more than one model surpassed this level of support, we considered the decision strategy with the highest $BF_m$ as the best fit to describe the participant's choices. 
Based on these results, we grouped participants according to the decision strategy that best described their behavior.

#### Performance
The main performance metric we focused was choice accuracy. 
We considered a choice as correct when the participant selected the cue ($C_L$ or $C_R$) with higher probability of winning (i.e., the higher sum of probabilistic weights), regardless of whether they received a "win" or "lose" feedback. 
We compared choice accuracy across T1 and T2, to determine the performance change when cue contingencies changed from T1 to T2. 
More specifically, we expected to find different patterns of T1-T2 performance change based on the underlying assumptions for each decision strategies as shown in Table \ref(tab:expectations). 
We also compared choice accuracy across decision strategies using only the data from T1, considering that the particular decision strategy used would be stabilized by T1. 
Additionally, we analyzed the total scored reward points and reaction time in T1 and compared them across decision strategies.

#### Fixation Patterns
To determine the information processing patterns, we quantified the fixations falling inside each cell of the grid cues during T1. To this end, we created four areas of interest (AOIs) within each cue, one for each of the four cells. 
We built these AOIs as squares with sides measuring two hundred pixels, centered on each of the equipment within a cell. We identified fixations using the I2CM algorithm from \citet{hessels_etal_2017_noiserobust_fixation}. 
Since our study focused on investigating the processing of attributes, we prioritized our analysis on the dissociation between the four features rather than across the two states within them. 
Therefore, we merged the AOIs from both cues, to reduce the number of AOIs to four, one per each feature. 
In other words, we considered a fixation as a hit for a particular AOI if it landed within the boundaries surrounding a feature (e.g., the helmet) regardless of whether it occurred on the left or right cue. 
This gave us an outcome variable with four levels corresponding to each of the AOIs centered on the features composing the cues. 

We analyzed the fixation patterns first by comparing the total number of fixations per trial across decision strategies to determine if the overall amount of information search varied across decision strategies. 
Next, we looked into the fixation patterns of each decision strategy to characterize the information processing for each strategy. 
For this purpose, we investigated the starting and stopping location of information search as well as the distribution of gaze across the difference features.
Therefore, we focused on three metrics: 1) the location of first fixation, 2) the location of last fixation, 3) the allocation of fixations across the four features.
We then contrasted these metrics with the expected patterns from each strategy, outlined in Table \ref{tab:ch2-expectations}. 
We selected these measures to capture the search rule representing the sampling sequence in the information space and the stopping rule that defines when the search stops \citep{gigerenzer_todd_1999_simple_heuristics}. 

With regards to the TTB strategy, we also considered the following four different decision scenarios (Fig. \ref{fig:ch2-scenarios}), considering that TTB should show a dynamic stopping point depending on when the user finds a discriminating feature between cues:

* When the cue states in the most informative feature (F1) are different (A).
* When cue states are the same in first feature, but different in the second (F2) (B).
* When cue states in both first and second feature are the same, but the third (F3) shows different states (C).
* All the features but the least important (F4) have the same state (D). 

We compared the total number of fixations and the location of last fixation across the four decision scenarios within the TTB users. 
We hypothesized that there will a progressive increase in the total number of fixations from scenario A to D, considering the search pattern this strategy should follow. 
We also expected that last fixations will be made mostly in F1 for scenario A; in F2 for scenario B; in F3 for scenario C, and in F4 for scenario D.

#### Statistical Models
As listed in Table \ref{tab:ch2-expectations}, some of the underlying assumptions predict no difference between comparisons made (e.g., see the predictions for Tallying). 
Furthermore, there were major differences in the number of users of each decision strategy (see \ref{fig:ch2-model-selection})
For these reasons, we used Bayesian parameter estimation on hierarchical linear models for statistical inference since they give us the possibility to find support for null differences and are also resilient to sample size problems \citep{rupp_etal_2004_bayes_not}. 
We used \citet{R-base} and the R-packages *brms* \citep{burkner_2018_advanced_bayesian} and *rstan* \citep{R-rstan} for this purpose. 
The full details on how we built the statistical models are included in Appendix A. 

For the performance data, since we have a binary outcome (i.e., correct/incorrect), we built a binomial model to estimate the probability of choosing the best cue, using decision strategy and decision phase (T1 vs. T2) as predictors. 
Additionally, we decided to look at the total points scored by participants during T1 as another performance measure. 
This metric is a count with high dispersion, which we decided to model using a hierarchical negative binomial model, which can properly fit this type of data \citep{cameron_trivedi_1998_regression_analysis}. 
In this model, we estimated the mean number of points obtained by each strategy, using the different decision strategies as a fixed effect. 
Furthermore, we also reviewed the response time during T1. 
For this metric, we considered the time between stimulus presentation and participant's choice as the response time within a trial. 
We then fitted these using a shifted log-normal model, which is suitable for response time data for which the earliest response is larger than 0 \citep{ranger_etal_2020_modeling_responses}. 
We estimated the mean response time for each strategy by incorporating them into the model as a fixed effect. 

For the total number of fixations per trial, which are also counts with high dispersion, we built a hierarchical negative binomial model. 
Similar to the score analysis, we estimated the mean total number of fixations per trial, using strategy as a fixed effect. 

For the analysis of fixation patterns (i.e., starting location, stopping location, gaze distribution), we assumed that a fixation could land on any of the four AOIs we defined, which corresponds to the four different features in the cue. 
This gives us a categorical outcome with four levels, which we analyzed using a hierarchical multinomial model with decision strategy as a fixed effect. 
We created separate multinomial models for the starting location, stopping location, and gaze distribution analysis. 
For the starting (and stopping) location, we estimated the probability that the initial (or final) fixation within a trial landed on each of the four AOIs.
In the case of the gaze distribution, we estimated the probability that a fixation would fall in each of the four AOIs within a trial.

For the analysis of performance and fixation patterns in the TTB strategy, we additionally tested for the effect of decision scenarios (A, B, C, D) that differ in terms of cue feature configurations. 
TTB is characterized by the dynamic stopping rule that assumes continued information search in descending order of feature importance, until the user finds a discriminating feature between cues. 
To test this behavior, we included a scenario predictor with four different levels (A, B, C, D) to the statistical models for both the total number of fixations and stopping location. 
All hierarchical models we described included random intercepts and/or slopes for all participants.

#### Statistical Comparisons
We used the posterior samples from the models to perform the comparisons between stages, decision groups and decision scenarios. In these comparisons, we want to assess if:

$${\theta_1} - {\theta_2} \neq 0$$
where $\theta_1$ and $\theta_2$ are the parameters of interest. 
For example, in the case of comparison across decision groups, we took the standardized posterior distributions from one group of interest and subtracted it from another, giving us a distribution of differences between the two groups. 
Then, we took the 89% highest density intervals (HDIs) from this distribution of differences \citep{mcelreath_2020_statistical_rethinking} to make inferences about any possible effects.
To do this, we used the probability of direction (PD, i.e., the proportion of the HDI that matches the sign of the median difference (MD)) \citep{makowski_etal_2019_indices_effect} as a benchmark to determine the presence of an effect. 
We interpreted any PD higher than 90% as an effect of interest. 
Finally, to assess whether the identified effects of interest are large enough to be meaningful, we defined a region of practical equivalence (ROPE) to a negligible effect size (Cohen's $\delta$ of 0.1).
This ROPE represents the threshold that the differences must surpass in either direction in order to determine a meaningful effect. 
Therefore, we measure the proportion of overlap between the HDI from our differences and the ROPE \citep{kruschke_2018_rejecting_accepting} to determine the significance of the effect. 
Differences with no overlap were interpreted as meaningful effects, overlaps below 25% as trends and overlaps beyond 25% as not meaningful.

# Results
## Model Classification
The results of model classifications are shown in Figure \ref(fig:ch2-model-selection), which reflect a great heterogeneity in the strategies used by the participants. 
Two of them seem to be predominant in both E1 and E2: 1st Only, which relies solely on the information from the most informative feature (model 1 in Fig. \ref(fig:ch2-model-selection)) and Partial Tallying, which shows partial integration of equal weights to the features (model 18 in Fig. \ref(fig:ch2-model-selection)). 
Partial Tallying only differs from its more compensatory parent (i.e., Tallying) in the number of features added up to determine the cue with more positive attributes. 
Participants classified in this group only integrate the first and second most informative features, both sharing the same level in the hierarchy, making both equally important to determine their choice. 
At the same time, the two most important features (F1 and F2) are explicitly identified as superior sources of information compared to the other two features. 

Besides these two strategies, we focused on several other decision schemes that would lead to successful performance in this task. 
The first candidate was the  TTB strategy, for which there is widespread knowledge of its efficiency in non-compensatory binary decision paradigms like this \citep{gigerenzer_gaissmaier_2011_heuristic_decision}. 
We additionally included the incomplete versions of TTB (models 19-21 in Fig. \ref(fig:ch2-model-selection)), and we grouped them all as the "Serial Search" group. 
Next, we chose the full implementation of Tallying (model 16 in Fig. \ref(fig:ch2-model-selection)) as a point of comparison with its partial version. 
Finally, we decided to also include participants whose choices showed a strong reliance in the second most informative feature (2nd Only, model 2 in Fig. \ref(fig:ch2-model-selection)), since this strategy should still yield reasonable performance on T1. 
The bottom bars in Fig. \ref(fig:ch2-model-selection) display the final set of decision strategies we chose to characterize in more detail and their frequency across E1 and E2 combined.

## Task Performance 
Task performance was analyzed in terms of the model estimates of the probability of choosing the best cue. We made comparisons across T1 and T2 within each decision strategy (Fig. \ref(fig:ch2-performance)A and B). 
In E1, we expected to see a decline in performance during T2 for any strategy that considers the ranking of the different features, and no change in strategies that do not use the ranking. 
In line with these predictions, we found a declining trend in performance from T1 to T2 for Serial Search (`r filter(rope_performance_change, experiment == "0") %>% report(strategy = "Serial Search")`) and Partial Tallying  (`r filter(rope_performance_change, experiment == "0") %>% report(strategy = "Partial Tallying")`). 
These users tend to rely on the feature rankings, so the drop in accuracy when the feature ranking disappears (i.e., T2) is consistent with the expectations for these strategies. 
However we did not observe a meaningful decrease in performance in users of 1st Only (`r filter(rope_performance_change, experiment == "0") %>% report(strategy = "1st Only")`) which is another strategy that uses rankings.
On the other hand, Tallying users who ignore the rankings and use only the highest number of winning attributes, showed no meaningful difference in performance (`r filter(rope_performance_change, experiment == "0") %>% report(strategy = "Tallying")`). Finally, we observed a positive trend in performance among 2nd Only users (`r filter(rope_performance_change, experiment == "0") %>% report(strategy = "2nd Only")`). 
The performance of this strategy, which only relies on the second most important feature (F2), may have been negatively impacted during T1, since the single feature they rely on (F2) may inaccurately override the weight of the most important feature (F1). Such negative impact is lifted in T2 when the features rankings are removed, resulting in relative performance improvement from T1 to T2. 

The shift in performance from T1 to T2 showed stronger effects in E2, where the feature rankings were reversed.
Here, we observed a significant decline in accuracy for Serial Search (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "Serial Search")`), Partial Tallying (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "Partial Tallying")`), and 1st Only (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "1st Only")`). 
This confirms that these strategies rely on raking the different pieces of information to a certain degree, an approach that is severely punished by the reverse ranking in T2. 
We also found a declining trend in 2nd Only (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "2nd Only")`). 
On the other hand, participants classified as Tallying users showed no meaningful difference (`r filter(rope_performance_change, experiment == "1") %>% report(strategy = "Tallying")`), confirming that they integrate all pieces of information with equal weight. 

After looking at the performance in each strategy separately, we also compared the T1 accuracy between strategies (Fig. \ref(fig:ch2-performance)C). 
We found that Serial Search users outperformed more frugal strategies (1st Only vs Serial Search: `r report(rope_performance_T1, comparison = "first_serial")`; 2nd Only vs Serial Search: `r report(rope_performance_T1, comparison = "second_serial")`).
Additionally, this strategy also outperformed more compensatory strategies (Serial Search vs Tallying `r report(rope_performance_T1, comparison = "serial_tally")`; Serial Search vs Partial Tallying `r report(rope_performance_T1, comparison = "serial_lazy")`). 

Analysis of the total score during T1 (Fig. \ref(fig:ch2-score)) showed a similar pattern. Serial Search outperforms Partial Tallying (`r report(rope_group_score, comparison = "serial_lazy")`) and 2nd Only (`r report(rope_group_score, comparison = "second_serial")`), and had a favorable trend against Tallying users (`r report(rope_group_score, comparison = "serial_tally")`). 
No meaningful difference was found when Serial Search was compared with 1st Only (`r report(rope_group_score, comparison = "first_serial")`).
Additionally, 1st Only showed an advantage against 2nd Only (`r report(rope_group_score, comparison = "first_second")`). 

We also compared response time across decision strategies (Fig. \ref(fig:ch2-time)). 
We found significantly faster responses for 1st Only compared to Tallying (`r report(rope_group_time, comparison = "first_tally")`), 2nd Only (`r report(rope_group_time, comparison = "first_second")`) and Partial Tallying (`r report(rope_group_time, comparison = "first_lazy")`. 
Additionally, there was a trend of faster response times when compared against Serial Search (`r report(rope_group_time, comparison = "first_serial")`). 
On the other hand, Serial Search only showed a trend of faster responses compared to Partial Tallying (`r report(rope_group_time, comparison = "serial_lazy")`). 

# Fixation patterns
## Total number of fixations
In an effort to examine the amount of information searched by the participants prior to making a choice, we first examined the total number of fixations per trial. 
We predicted that strategies relying on a smaller number of features (e.g., 1st Only and 2nd Only), would need fewer number of fixations to complete their information search. 
As shown in Figure \ref(fig:ch2-fixcount), 1st Only users made significantly less fixations than the compensatory strategies (1st Only vs Tallying: `r report(rope_group_fixcount, comparison = "first_tally")`; 1st Only vs Partial Tallying: `r report(rope_group_fixcount, comparison = "first_lazy")`). 
Similarly, 2nd Only displayed trends of making less fixations than compensatory strategies (2nd Only vs Tallying: `r report(rope_group_fixcount, comparison = "second_lazy")`; 2nd Only vs Partial Tallying: `r report(rope_group_fixcount, comparison = "second_tally")`).
2nd Only users also showed a trend suggesting that they make more fixations than 1st Only users (`r report(rope_group_fixcount, comparison = "first_second")`), even though both strategies rely on a single feature.
Serial Search displayed trends of making less fixations than Tallying (`r report(rope_group_fixcount, comparison = "serial_tally")`) and Partial Tallying (`r report(rope_group_fixcount, comparison = "serial_lazy")`). 
Interestingly, we found no meaningful difference between Serial Search and the single feature strategies (1st Only vs Serial Search: `r report(rope_group_fixcount, comparison = "first_serial")`; 2nd Only vs Serial Search: `r report(rope_group_fixcount, comparison = "second_serial")`), which highlights the sampling frugality of this lexicographic strategy, even when compared with strategies relying on a single piece of information.

For serial search users, we additionally compared the total number of fixations across the four decision scenarios.
As we can see in Figure \ref(fig:ch2-fixcount-scenario) this strategy progressively sampled more as the differentiating feature was located further down in the ranking of importance (scenario A vs B: `r report(rope_fixcount_scenario_comparison, strategy = "Serial Search", comparison = "A_B")`; scenario B vs C: `r report(rope_fixcount_scenario_comparison, strategy = "Serial Search", comparison = "B_C")`; with no meaningful difference between scenario C vs D: `r report(rope_fixcount_scenario_comparison, strategy = "Serial Search", comparison = "C_D")`). 
This behavior is for the most part in line with the expectations from the TTB strategy.

## Strategy specific patterns
After surveying the overall amount of information search across decision strategies, we looked into the strategy-specific search patterns and tested for the underlying assumption as listed in Table \ref(tab:ch2-expectations). 
To refer to the starting location, stopping location and gaze distribution, we will use the terms first fixation, last fixation, and allocation of fixations in the description of fixation patterns.
Below we discuss the results from each strategy separately.

### 1st Only
This decision strategy assumes that participants would only rely on the most informative feature (F1) to make their choices.
Therefore, we expected a consistent preference toward the most informative feature in terms of the location of both first and last fixation, as well as the allocation of fixations. 

In line with our expectations, we found that these participants were more likely to direct their first fixation (F1 vs all: `r report(rope_first_domain, strategy = "1st Only")`) and last fixation (F1 vs all: `r report(rope_last_domain, strategy = "1st Only")`) towards the most important feature, relative to the others (Fig. \ref(fig:ch2-fixation-patterns)A). 
The pattern repeats when we take a look at the allocation of fixations across the four features (F1 vs all: `r report(rope_proportion_domain, strategy = "1st Only")`). 
Overall, these results suggest a strong focus toward the most important piece of information in these participants, which matches with our predictions on this strategy.

### 2nd Only
This strategy also considers a single feature to make a choice, but in this case, it relies on the second most informative feature (F2).
We observed a trend of directing first fixations toward the second most important feature more often than to other features (F2 vs all: `r report(rope_first_domain, strategy = "2nd Only")`) (Fig. \ref(fig:ch2-fixation-patterns)B). 
We identified a similar pattern in the last fixations (F2 vs all: `r report(rope_last_domain, strategy = "2nd Only")`) and allocation of fixations (F2 vs all: `r report(rope_proportion_domain, strategy = "2nd Only")`).
These results demonstrate a reliance on the second most informative feature, in line with the choice model, although we see a greater heterogeneity in fixation patterns compared to the 1st Only strategy.

### Serial Search
The two strategies described above use static fixation patterns, always preferring a single feature over the others. 
For users of strategies that rely on conditional sampling, such as in the case of Serial Search, the preferred piece of information changes depending on the feature states presented in a given trial. 
In particular, Serial Search should start the information search at the most important feature, comparing it between cues and only continue sampling if this feature does not discriminate between the two cues. 
Therefore, we expect an overall preference for the most important feature as the search starting point. 
In terms of the overall gaze distribution, we predicted differences reflecting the feature rankings, since the information search from this group should follow the features' importance in descending order.

In line with our expectations, we found that they were more likely to direct their first fixation toward the most important feature relative to the others (F1 vs all: `r report(rope_first_domain, strategy = "Serial Search")`) (Fig. \ref(fig:ch2-fixation-patterns)C). 
Looking at the allocation of fixations, the differences between features reflected their rankings, except for the least important features (F1 vs F2: `r report(rope_proportion_domain, strategy = "Serial Search", comparison = "d1_2")`; F2 vs F3: `r report(rope_proportion_domain, strategy = "Serial Search", comparison = "d2_3")`; F3 vs F4: `r report(rope_proportion_domain, strategy = "Serial Search", comparison = "d3_4")`). 

For their last fixations, we had different expectations across decision scenarios (Fig. \ref(fig:ch2-scenarios), reflecting the dynamic stopping rule for this strategy. 
Information search should end at the most important feature in scenario A, while it should end at the second, third, and fourth ranked features for scenarios B, C, D, respectively. 
Most of these predictions matched the results of the last fixations analysis, except for scenario D, where we only found a trend favoring the least important feature (scenario A, F1 vs all: `r report(rope_last_scenario_comparison, strategy = "Serial Search", scenario = "0", comparison = "d1_all")`; scenario B, F2 vs all: `r report(rope_last_scenario_comparison, strategy = "Serial Search", scenario = "1", comparison = "d2_all")`; scenario C, F3 vs all: `r report(rope_last_scenario_comparison, strategy = "Serial Search", scenario = "2", comparison = "d3_all")`; scenario D, F4 vs all: `r report(rope_last_scenario_comparison, strategy = "Serial Search", scenario = "3", comparison = "d4_all")`; Fig. \ref(fig:ch2-serial-last)).

To determine the specificity of these fixation patterns reflecting the dynamic stopping point in Serial Search, we tested for the presence of this pattern in other decision strategies. 
When the above-described contrasts were applied to last fixations in all other strategy groups, we found no strong evidence suggesting the presence of such patterns (Fig. \ref(fig:ch2-serial-last-comparison)).
Only Partial Tallying showed last fixation patterns that resembles Serial Search in scenarios B(F2 vs all: `r report(rope_last_scenario_comparison, strategy = "Partial Tallying", scenario = "1", comparison = "d2_all")`), C(F3 vs all: `r report(rope_last_scenario_comparison, strategy = "Partial Tallying", scenario = "2", comparison = "d3_all")`), and D(scenario D, F4 vs all: `r report(rope_last_scenario_comparison, strategy = "Partial Tallying", scenario = "3", comparison = "d4_all")`) and a trend in scenario A(scenario A, F1 vs all: `r report(rope_last_scenario_comparison, strategy = "Partial Tallying", scenario = "0", comparison = "d1_all")`).

### Tallying
This strategy uses all cue features; however, it assigns them equal importance and counts the number of features favoring each cue. 
The option with a higher number of positive attributes becomes selected. 
Considering this, we should see no difference in allocation of fixations across the different features, reflecting an equal consideration of all of them.

In line with our expectations, users of this strategy showed a fairly uniform distribution of where the first fixation is made. 
However, there were trends favoring the most important feature (F1 vs F2: `r report(rope_first_domain, strategy = "Tallying", comparison = "d1_2")`; F1 vs F3: `r report(rope_first_domain, strategy = "Tallying", comparison = "d1_3")`; F1 vs F4: `r report(rope_first_domain, strategy = "Tallying", comparison = "d1_4")`) (Fig. \ref(fig:ch2-fixation-patterns)D). 
We found similar trends in their last fixations (F1 vs F2: `r report(rope_last_domain, strategy = "Tallying", comparison = "d1_2")`; F1 vs F3: `r report(rope_first_domain, strategy = "Tallying", comparison = "d1_3")`; F1 vs F4: `r report(rope_first_domain, strategy = "Tallying", comparison = "d1_4")`), as well as in allocation of fixations (F1 vs F2: `r report(rope_proportion_domain, strategy = "Tallying", comparison = "d1_2")`; F1 vs F3: `r report(rope_proportion_domain, strategy = "Tallying", comparison = "d1_3")`; F1 vs F4: `r report(rope_proportion_domain, strategy = "Tallying", comparison = "d1_4")`).
Overall, these participants seem to be aware of the ranking among features and use it to guide their information search to some extent, despite the fact that they are integrating all the features from the cues. 

### Partial Tallying
This decision strategy integrates the first and second most informative features (F1 and F2) with equal weights, which are considered as superior sources of information than the other two features (F3 and F4).
Reflecting this, we should see no difference between the first and second feature, given its equal subjective importance and we should find a difference between the two most and two least important features.

Our results on the first fixation did not support our predictions. 
There was a trend showing difference between the first and second most informative features (`r report(rope_first_domain, strategy = "Partial Tallying", comparison = "d1_2")`) (Fig. \ref(fig:ch2-fixation-patterns)E). 
Additionally, the difference between the two most important features, compared to the two least informative features was not meaningful (F1 and F2 vs F2 and F4: `r report(rope_first_domain, strategy = "Partial Tallying", comparison = "d12_34")`). 
We found results more in line with our expectations in the last fixations. 
There was no meaningful difference between the top two features (F1 vs F2: `r report(rope_last_domain, strategy = "Partial Tallying", comparison = "d1_2")`) or the two least important features (F3 vs F4: `r report(rope_last_domain, strategy = "Partial Tallying", comparison = "d3_4")`), and a trend favoring the two most important features compared to the least important ones (F1 and F2 vs F3 and F4: `r report(rope_last_domain, strategy = "Partial Tallying", comparison = "d12_34")`). 
The allocation of fixations revealed similar patterns, showing no meaningful difference between the two most important features (F1 vs F2: `r report(rope_proportion_domain, strategy = "Partial Tallying", comparison = "d1_2")`), no difference between the least important ones (F3 vs F4: `r report(rope_proportion_domain, strategy = "Partial Tallying", comparison = "d3_4")`), and a trend favoring the former pair (F1 and F2 vs F3 and F4: `r report(rope_proportion_domain, strategy = "Partial Tallying", comparison = "d12_34")`). 
Overall, this incomplete implementation of the Tallying strategy shows heterogeneity in the starting and stopping points of its information search, demonstrating that they consult with more feature than expected.

## Comparisons across strategies 
Finally, to evaluate the uniqueness of the fixation patterns in Serial Search users, we compared their allocation of fixations among the four features against the gaze distribution from the other strategies.
To this end, we conducted a series of 2 (strategy: Serial Search vs. other) x 4 (feature: F1 vs. F2 vs. F3 vs. F4) ANOVAs to compare the Serial Search group to other strategies. 
We found a significant strategy by feature interaction in the comparisons with 2nd Only (`r apa_proportion_ss_2nd$statistic$strategy_rank`), Tallying (`r apa_proportion_ss_tl$statistic$strategy_rank`), and Partial Tallying (`r apa_proportion_ss_ptl$statistic$strategy_rank`), indicating dissociable allocation of sampling efforts across features (Fig. \ref(fig:ch2-interactions)).

# Discussion

Real-world decision making under uncertainty is constrained by incompleteness of the available information and the computational limits of the agent \citep{simon_1955_behavioral_model, simon_1986_theories_bounded}. 
Acknowledging these constraints, \citet{simon_1955_behavioral_model} suggested that our decisions are made under "bounded rationality". 
This presents a challenge to the underlying assumptions made in classical definitions of rationality, which say that optimal agents should rely on the use of logic and probability. 
Reflecting these limitations, Simon proposes that real-world rational behavior is shaped by âscissors whose two blades are the structure of the task environment and the computational capabilities of the actorâ \citep{simon_1990_invariants_human}. 
In line with the perspective of bounded rationality, there is an extensive body of research proposing that humans use decision strategies that aim to reach good enough results (i.e., satisficing) rather than looking to achieve the best possible outcome \citep{gigerenzer_goldstein_1996_reasoning_fast, simon_1955_behavioral_model, simon_1956_rational_choice}. 
Such strategies, known as heuristics, allow us to overcome time, knowledge and/or computational constraints. 
Heuristics can ignore certain attributes to reduce the time and effort required in reviewing the information, while still allowing the agent to achieve accurate decisions  \citep{gigerenzer_gaissmaier_2011_heuristic_decision}.

The current study aimed to investigate the decision strategies used during a probabilistic classification task, which emulates the uncertainty in real-life decision making, where a choice is made based on multiple attributes of unknown validities.
Using model classification, we determined the commonly adopted decision strategies in our task environment. 
After identifying the decision strategies, we studied the pattern of pre-decisional information processing with the purpose of verifying the underlying assumptions of the identified decision strategies.  

Our model classification results demonstrated a noticeable heterogeneity in strategic use. 
We found five main decision strategies implemented by our participants: 1st Only which relies on the most important feature, 2nd Only which relies on the second most important feature, Serial Search which encompass TTB and strategies using a similar lexicographic search pattern, Tallying which assigns equal weights to all features and counts the number of winning states, and Partial Tallying which similarly assigns equal weights, but only considers the two most important features. 

These model classification results were confirmed by the performance analysis between testing phases.
We found a decline in performance from T1 to T2 in strategies using the feature rankings, such as Serial Search and 1st Only. 
Such effect was more pronounced in E2 where the feature rankings were completely inverted. 
On the other hand, Tallying users who do not rely on feature rankings remained unaffected by the shift in cue contingencies.

Given the non-compensatory decision environment presented in our task which is favorable to lexicographic strategies such as the TTB heuristic \citep{martignon_hoffrage_1999_why_does, martignon_hoffrage_2002_fast_frugal}, we expected Serial Search to be used most frequently among participants. 
However, this was not the case.
Instead, the most common strategies were 1st Only and Partial Tallying. 
One possible explanation for why Serial Search was not the most preferred strategy can be found in the discrepancy between our task environment and that of other studies.
Unlike our study, in most previous works showing the prevalence of serial search, the participants were explicitly presented with the weights and rankings of the different features (e.g., \citealt{broder_2000_assessing_empirical, dieckmann_rieskamp_2007_influence_information, rieskamp_hoffrage_2008_inferences_time}) or they were indicated of the presence of rankings \citep{glockner_etal_2012_processing_differences, oh-descher_etal_2017_probabilistic_inference, oh_etal_2016_satisficing_splitsecond}. 
These explicit rankings may inevitably bias agent's search patterns and the decision strategies to those assuming the rankings, such as TTB \citep{juslin_persson_2002_probabilities_exemplars}.
Additionally, instructing participants to make fast and accurate responses in our task could have led them to sample less pieces of information, which is a phenomenon observed in multiple studies incorporating explicit time pressure (e.g., \citealt{bobadilla-suarez_love_2018_fast_frugal, oh-descher_etal_2017_probabilistic_inference, oh_etal_2016_satisficing_splitsecond, rieskamp_hoffrage_2008_inferences_time}). 

The fact that Serial Search was not the most popular strategy was all the more surprising given that both choice accuracy and reward points earned demonstrate its superiority in performance. 
Additionally, we did not find any meaningful differences in sampling efficiency between Serial Search and 1st Only users, reflected in the average number of fixations per trial. 
When compared to Partial Tallying which was another popular strategy, Serial Search clearly dominated in terms of performance and showed a more frugal trend in terms of amount of information search. 
These results collectively suggest that strategic selection may not necessarily depend solely based on improving performance search efficiency. 
Importantly, the comparison of response time across strategies revealed that 1st Only showed the fastest response across all strategies, even compared to Serial Search (although only a trend). 
The fact that the fast response of 1st Only did not result in sacrificing the overall performance (i.e., no meaningful difference in choice accuracy and reward points compared to Serial Search) nor the frugality of information search, makes this strategy all the more appealing as reflected in its popularity.      

Beyond comparing the overall performance and amount of sampling across strategies, we further investigated the information processing patterns of each strategy by studying location of their first fixation, last fixations, and the allocation of fixations in every trial.
These metrics capture the specific starting and stopping points of their search, as well as the gaze distribution before making a choice \citep{gigerenzer_todd_1999_simple_heuristics}. 
These analyses revealed that 1st Only and 2nd Only users indeed showed a clear preference to the single feature they relied on (first and second most important features, respectively). 
We also found that there was some heterogeneity in the adherence to this single attribute, such that they also made first and last fixations on other features to a measurable degree (more prominent in 2nd Only).
This may imply that while they only seem to use the information from a single feature to make their choice, they still review other features during the process.
The fixations made to other features may also reflect the scanning of attributes to identify the single feature they rely on.

For Serial Search we expected a more complex fixation pattern, compared to the single feature strategies. 
Specifically, the stopping rule adopted by this heuristic assumes a clear understanding and use of the feature rankings to its advantage. 
In our data, Serial Search users indeed showed a clear dissociation across features reflecting preference in the order of feature rankings shown both in the starting point and gaze distribution analysis. 
The comparison of gaze distribution across decision strategies also confirmed the greatest adherence to feature rankings in Serial Search. 
Additionally, the last fixation analysis across the four decision scenarios (i.e., A, B, C, D), suggested a dynamic change in their stopping point based on where the discerning feature lies, a pattern that was not consistently observed in other strategies. 
Together with the results from the total number of fixations, these results indicate that Serial Search users effectively finish their search when the discerning feature is found, which results in an equivalent level of sampling frugality as 1st Only.

Moving on to compensatory strategies, in Tallying users, we were expecting to see no difference in the overall allocation of fixations and no preference for the starting or ending point of their search. 
This is because this strategy weighs all the features equally, and therefore, we should not see any distinction between them.
Unlike our prediction however, the fixation patterns indicated that these participants are aware of the feature rankings to some extent. 
The fixation patterns reflected a preference for the most important feature. 
We found a similar pattern among Partial Tallying users. 
These results show that while these individuals may be aware of feature rankings, they donât necessarily rely on the different cue weights to make their choice. 

Collectively, the analysis of strategy-specific fixation patterns showed a correspondence between the decision models determined based on model classification and their predicted information sampling patterns. 
However, we also found that all strategies commonly demonstrated awareness in feature ranking to some extent, even those not using them to guide their choices in the end. 
This suggests the prominence of feature rankings in the task environment, which participants could readily use, such as it happened in those adopting Serial Search. 
Despite the prominence of feature rankings, and their ability to identify them, not all participants adopted a strategy that fully takes advantage of them. 
This entails that the usage of lexicographic strategies in this type of decision environments not only depends on the awareness of the ranking or in the execution of certain sampling patterns accordingly, but also in a more conscious effort to incorporate them into a decision rule.

It is worthwhile to discuss some limitations of our study. 
First, as a mere consequence of the unique combinations of features in making up a pair of grid cues, the number of trials that fall under decision scenarios C and D (where the third and fourth most important features are the discriminating piece of information, respectively) are fewer than for scenarios A and B. 
This limits participants' exposure to them, and the validity of results comparing across these scenarios. 
Furthermore, it introduces a lot of variability in the estimation of the fixation benchmarks in those instances. 
Another limitation from our paradigm comes from the adaptive time thresholds, which was implemented to promote use of decision heuristics. 
Under such environment, participants who coincidentally started responding fast would experience a huge time pressure in the following trials to get the bonus, leading them to use more fast and frugal strategies, such as 1st Only and 2nd Only.
This time pressure implementation might have swayed the participants away from using strategies that guarantee higher accuracy, such as Serial Search. 
In addition, we do not venture to analyze what possible information integration process could be present across different strategies, although recent findings suggest the possibility of a single mechanism, regardless of the pattern of information search \citep{glockner_etal_2014_what_adaptive}.
Finally, our model classifications approach assumes the implementation of a single model throughout the task. 
Although one can argue that the participants' behavior very likely consolidated after the training stage, considering the large number of trials presented in it, there is still the possibility that some participants may be using a mixture of strategies. 
This could explain the non-conclusive fixation patterns in some of the metrics for Partial Tallying and 2nd Only. 

In summary, we present evidence of the adaptability of lexicographic heuristics in non-compensatory environments, outperforming users of other strategies.
Additionally, we show the plausibility of its predicted search patterns including the dynamic stopping rule. 
Despite the superiority in performance, we also show that this lexicographic strategies are not commonly found among participants. 
Instead, strategies implementing simpler rules that rely on fewer number of features were used more often. 
Such preference towards the simpler strategies may result from the fast response they allow despite reaching equivalent levels of efficiency in terms information sampling, which is critical to perform well with the lax-adaptive time pressure implemented in our task. 
Finally, we showed the importance of process tracing methodologies, such as eye-tracking, to corroborate some of the information processing patterns defined by formal decision models, such as the dynamic stopping rule. 
Nonetheless, the analysis of eye-tracking patterns also demonstrated that the sampling of information and the pattern in which it happens do not necessarily translate into their choice strategy. 

\newpage

## Tables and Figures
\renewcommand{\arraystretch}{1}

\singlespacing

```{r ch2-expectations}
comparison_table_A <- tibble(
  "Strategy" = c("First\nonly", "Second\nonly", "Serial\nSearch", "Tallying", "Partial\nTallying"),
  
  "E1" = c(
    "Decrease in\nperformance with\nrespect to T1", 
    "-", 
    "Decrease in\nperformance with\nrespect to T1", 
    "No meaningful\nchange in performance\nwith respect to T1", 
    "-"),
  
  "E2" = c(
    "Decrease in\nperformance with respect to T1", 
    "-",
    "Decrease in\nperformance with respect to T1",
    "No meaningful change in performance with respect to T1",
    "-"),
  
  "Starting point" = c(
    "More fixations directed to F1 compared to other features", 
    "More fixations directed to F2 compared to other features", 
    "More fixations directed to F1 compared to other features",
    "No meaningful difference between fixations across features", 
    "More fixations directed toward F1-F2, but no meaningful difference between them")
  
  )

kable(comparison_table_A, 
      format = "latex", 
      booktabs = T,
      caption = "Decision strategies' expected behavior") |> 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) |> 
  row_spec(row = 0, align = "c") |> 
  kable_styling(full_width = TRUE) |> 
  column_spec(1, width = "2cm")

```

\newpage

```{r ch2-expectations-cont}
comparison_table_B <- tibble(
  "Strategy" = c("First\nonly", "Second\nonly", "Serial\nSearch", "Tallying", "Partial\nTallying"),

  "Gaze distribution" = c(
    "More fixations directed to F1 compared to other features",
    "More fixations directed to F1 compared to other features",
    "Linear trend in the allocation of fixations across features in descending order of importance",
    "No meaningful difference between fixations across features",
    "More fixations directed toward F1-F2, but no meaningful difference between them"),

  "Stopping point" = c(
    "More fixations directed to F1 compared to other features",
    "More fixations directed to F1 compared to other features",
    "Linear trend in the allocation of fixations across features in descending order of importance",
    "No meaningful difference between fixations across features",
    "More fixations directed toward F1-F2, but no meaningful difference between them"),

  "Decision scenarios" = c(
    "-",
    "-",
    "A: more last fixations to F1 vs other features; B: more last fixations to F2 vs other features; C: more last fixations to F3 vs other features; D: more last fixations to F4 vs other features",
    "-",
    "-")
  )

kable(comparison_table_B, 
      format = "latex", 
      booktabs = T,
      caption = "(Cont.) Decision strategies' expected behavior") |> 
  kable_styling(latex_options = c("HOLD_position", "scale_down")) |> 
  row_spec(row = 0, align = "c") |> 
  kable_styling(full_width = TRUE) |> 
  column_spec(1, width = "2cm")

```


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{../../figures/ch02_task_design.png}
    \caption[Probabilistic classification task general setup.]{(A) Stimulus organization: We will present two grids on the left and right side of the screen. Each grid contains four pieces of equipment (features), with different brands overlayed on top of them. All the equipment and brands will maintain their position across participants, but we will assign the allocation of weights semi randomly. P1 and T1 use the same contingencies, but we will introduce changes for T2. During E1, we will assign all items with the same weights. For E2 we will switch the probabilistic weights across features, inverting their relative importance. (B) Task sequence: presentation timing of trial stages. (C) Feedback: We can see a display of all the possible feedback messages that participants will see, as well as the criteria that triggers them.}
    \label{fig:ch2-task-setup}
\end{figure}

\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{../../figures/ch02_decision_models.png}
    \caption[Models representing decision strategies.]{The circles represent the features consulted by each model. In models 1-15, filled circles indicate the feature(s) used by each strategy. For models 16-18 we created a model that integrated the features in a single piece of information (i.e., Tallying). Models 19-21 consider a Serial Search for information. On the bottom we show the final list of decision models we used for the rest of the analysis.}
    \label{fig:ch2-model-configuration}
\end{figure}

\newpage

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{../../figures/ch02_decision_scenarios.png}
    \caption[Decision scenarios.]{These scenarios are created based on the dynamic search rule from TTB according to the best discriminating feature between cues. In scenario A, where the most predictive feature is different between the two cues (red and light red squares), participants using this strategy should only focus on this feature, ignoring the rest. In scenario B, the most predictive feature is the same between cues (gray squares) but the second most predictive is different, therefore, they should stop there and ignore the rest of the features. This patter will continue, until reaching scenario D, where all the features are the same, except for the fourth most predictive, in that case, the TTB users should scan all the cue and reach their choice until the very end.}
    \label{fig:ch2-scenarios}
\end{figure}

\newpage
\doublespacing


